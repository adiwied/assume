# SPDX-FileCopyrightText: ASSUME Developers
#
# SPDX-License-Identifier: AGPL-3.0-or-later

base_inspect_init_try:
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: True

  learning_config:
    continue_learning: False
    trained_policies_save_path: null
    max_bid_price: 100
    algorithm: matd3
    device: cpu
    learning_rate: 0.0003
    validation_episodes_interval: 10 # after how many episodes the validation starts and the policy is updated
    training_episodes: 100
    gradient_steps: 10
    matd3:
      actor_architecture: mlp
      train_freq: 24h # how often write_to_learning_role gets called
      episodes_collecting_initial_experience: 3
      batch_size: 64
      gamma: 0.99
      noise_sigma: 0.1
      noise_scale: 1
      noise_dt: 1

  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear


base_ppo_sl_si_lstd_bb_long_98_1:
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: True

  learning_config:
    continue_learning: False
    trained_policies_save_path: null
    max_bid_price: 100
    algorithm: ppo
    device: cpu
    learning_rate: 0.0003 # default learning rate
    learning_rate_schedule: linear
    validation_episodes_interval: 5 # after how many episodes the validation starts and the policy is updated
    training_episodes: 100
    gradient_steps: 10
    matd3:
      actor_architecture: mlp
      train_freq: 24h # how often write_to_learning_role gets called
      episodes_collecting_initial_experience: 3
      batch_size: 64
      gamma: 0.99
      noise_sigma: 0.1
      noise_scale: 1
      noise_dt: 1
    ppo: 
      actor_architecture: dist
      train_freq: 33h # how often write_to_learning_role gets called
      gamma: 0.99 # Discount factor for future rewards
      clip_ratio: 0.05  # Clipping parameter for policy updates
      vf_coef: 0.75  # Value function coefficient in the loss function
      entropy_coef: 0.005  # Entropy coefficient for exploration
      max_grad_norm: 0.3  # Gradient clipping value
      gae_lambda: 0.95  # GAE lambda for advantage estimation
      batch_size: 33  # Batch size for each update, if mini-batch approach is used (currently not implemented)
      


  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear

base_mappo_try_speedup_EMA000_33bs_const_lr0001_vc_01:
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: True

  learning_config:
    experiment_group: "comparison_interim_presentation"
    experiment_name: 2b_mappo
    algo_detail: nonormlr0003vclip005
    continue_learning: False
    trained_policies_save_path: null
    max_bid_price: 100
    algorithm: ppo
    device: cpu
    learning_rate: 0.0001 # default learning rate
    #learning_rate_schedule: None
    validation_episodes_interval: 5 # after how many episodes the validation starts and the policy is updated
    training_episodes: 100
    gradient_steps: 5
    matd3:
      actor_architecture: mlp
      train_freq: 24h # how often write_to_learning_role gets called
      episodes_collecting_initial_experience: 3
      batch_size: 64
      gamma: 0.99
      noise_sigma: 0.1
      noise_scale: 1
      noise_dt: 1
    ppo: 
      actor_architecture: dist
      train_freq: 33h # how often write_to_learning_role gets called
      gamma: 0.99 # Discount factor for future rewards
      clip_ratio: 0.05  # Clipping parameter for policy updates
      vf_coef: 0.75  # Value function coefficient in the loss function
      entropy_coef: 0.005  # Entropy coefficient for exploration
      max_grad_norm: 0.3  # Gradient clipping value
      gae_lambda: 0.95  # GAE lambda for advantage estimation
      batch_size: 33  # Batch size for each update, if mini-batch approach is used (currently not implemented)
      # Adjust agent information structure
      share_critic: false
      use_base_bid: false
      learn_std: true
      public_info: true
      individual_values: true


  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear

base_mappo_Step9:
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: True

  learning_config:
    experiment_group: "comparison_interim_presentation"
    experiment_name: 2b_mappo
    algo_detail: nonormlr0003vclip005
    continue_learning: False
    trained_policies_save_path: null
    max_bid_price: 100
    algorithm: ppo
    device: cpu
    learning_rate: 0.0003 # default learning rate
    learning_rate_schedule: linear
    validation_episodes_interval: 5 # after how many episodes the validation starts and the policy is updated
    training_episodes: 100
    gradient_steps: 10
    matd3:
      actor_architecture: mlp
      train_freq: 24h # how often write_to_learning_role gets called
      episodes_collecting_initial_experience: 3
      batch_size: 64
      gamma: 0.99
      noise_sigma: 0.1
      noise_scale: 1
      noise_dt: 1
    ppo: 
      actor_architecture: dist
      train_freq: 144h # how often write_to_learning_role gets called
      gamma: 0.99 # Discount factor for future rewards
      clip_ratio: 0.05  # Clipping parameter for policy updates
      vf_coef: 0.75  # Value function coefficient in the loss function
      entropy_coef: 0.005  # Entropy coefficient for exploration
      max_grad_norm: 0.3  # Gradient clipping value
      gae_lambda: 0.95  # GAE lambda for advantage estimation
      batch_size: 144  # Batch size for each update, if mini-batch approach is used (currently not implemented)
      # Adjust agent information structure
      share_critic: false
      use_base_bid: false
      learn_std: true
      public_info: true
      individual_values: true


  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear


base_mappo_Step10:
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: True

  learning_config:
    experiment_group: "comparison_interim_presentation"
    experiment_name: 2b_mappo
    algo_detail: nonormlr0003vclip005
    continue_learning: False
    trained_policies_save_path: null
    max_bid_price: 100
    algorithm: ppo
    device: cpu
    learning_rate: 0.0003 # default learning rate
    learning_rate_schedule: linear
    validation_episodes_interval: 5 # after how many episodes the validation starts and the policy is updated
    training_episodes: 100
    gradient_steps: 10
    matd3:
      actor_architecture: mlp
      train_freq: 24h # how often write_to_learning_role gets called
      episodes_collecting_initial_experience: 3
      batch_size: 64
      gamma: 0.99
      noise_sigma: 0.1
      noise_scale: 1
      noise_dt: 1
    ppo: 
      actor_architecture: dist
      train_freq: 144h # how often write_to_learning_role gets called
      gamma: 0.99 # Discount factor for future rewards
      clip_ratio: 0.05  # Clipping parameter for policy updates
      vf_coef: 0.75  # Value function coefficient in the loss function
      entropy_coef: 0.005  # Entropy coefficient for exploration
      max_grad_norm: 0.3  # Gradient clipping value
      gae_lambda: 0.95  # GAE lambda for advantage estimation
      batch_size: 144  # Batch size for each update, if mini-batch approach is used (currently not implemented)
      # Adjust agent information structure
      share_critic: false
      use_base_bid: false
      learn_std: true
      public_info: true
      individual_values: true


  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear


base_mappo_ival_wandb_Step:
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: True

  learning_config:
    experiment_group: "comparison_interim_presentation"
    continue_learning: False
    trained_policies_save_path: null
    max_bid_price: 100
    algorithm: ppo
    device: cpu
    learning_rate: 0.0003 # default learning rate
    learning_rate_schedule: linear
    validation_episodes_interval: 5 # after how many episodes the validation starts and the policy is updated
    training_episodes: 100
    gradient_steps: 10
    matd3:
      actor_architecture: mlp
      train_freq: 24h # how often write_to_learning_role gets called
      episodes_collecting_initial_experience: 3
      batch_size: 64
      gamma: 0.99
      noise_sigma: 0.1
      noise_scale: 1
      noise_dt: 1
    ppo: 
      actor_architecture: dist
      train_freq: 144h # how often write_to_learning_role gets called
      gamma: 0.99 # Discount factor for future rewards
      clip_ratio: 0.05  # Clipping parameter for policy updates
      vf_coef: 0.75  # Value function coefficient in the loss function
      entropy_coef: 0.005  # Entropy coefficient for exploration
      max_grad_norm: 0.3  # Gradient clipping value
      gae_lambda: 0.95  # GAE lambda for advantage estimation
      batch_size: 144  # Batch size for each update, if mini-batch approach is used (currently not implemented)
      # Adjust agent information structure
      share_critic: false
      use_base_bid: false
      learn_std: true
      public_info: true
      individual_values: true

  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear
      
base_ippo_ival_wandb_nonorm2:
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: True

  learning_config:
    experiment_group: "comparison_interim_presentation"
    continue_learning: False
    trained_policies_save_path: null
    max_bid_price: 100
    algorithm: ppo
    device: cpu
    learning_rate: 0.0003 # default learning rate
    learning_rate_schedule: linear
    validation_episodes_interval: 5 # after how many episodes the validation starts and the policy is updated
    training_episodes: 100
    gradient_steps: 10
    matd3:
      actor_architecture: mlp
      train_freq: 24h # how often write_to_learning_role gets called
      episodes_collecting_initial_experience: 3
      batch_size: 64
      gamma: 0.99
      noise_sigma: 0.1
      noise_scale: 1
      noise_dt: 1
    ppo: 
      actor_architecture: dist
      train_freq: 144h # how often write_to_learning_role gets called
      gamma: 0.99 # Discount factor for future rewards
      clip_ratio: 0.05  # Clipping parameter for policy updates
      vf_coef: 0.75  # Value function coefficient in the loss function
      entropy_coef: 0.005  # Entropy coefficient for exploration
      max_grad_norm: 0.3  # Gradient clipping value
      gae_lambda: 0.95  # GAE lambda for advantage estimation
      batch_size: 144  # Batch size for each update, if mini-batch approach is used (currently not implemented)
      # Adjust agent information structure
      share_critic: false
      use_base_bid: false
      learn_std: true
      public_info: false
      individual_values: true

  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear



base_ippo_ival_wandb_paramshare2:
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: True

  learning_config:
    experiment_group: "comparison_interim_presentation"
    continue_learning: False
    trained_policies_save_path: null
    max_bid_price: 100
    algorithm: ppo
    device: cpu
    learning_rate: 0.0003 # default learning rate
    learning_rate_schedule: linear
    validation_episodes_interval: 5 # after how many episodes the validation starts and the policy is updated
    training_episodes: 100
    gradient_steps: 10
    matd3:
      actor_architecture: mlp
      train_freq: 24h # how often write_to_learning_role gets called
      episodes_collecting_initial_experience: 3
      batch_size: 64
      gamma: 0.99
      noise_sigma: 0.1
      noise_scale: 1
      noise_dt: 1
    ppo: 
      actor_architecture: dist
      train_freq: 144h # how often write_to_learning_role gets called
      gamma: 0.99 # Discount factor for future rewards
      clip_ratio: 0.05  # Clipping parameter for policy updates
      vf_coef: 0.75  # Value function coefficient in the loss function
      entropy_coef: 0.005  # Entropy coefficient for exploration
      max_grad_norm: 0.3  # Gradient clipping value
      gae_lambda: 0.95  # GAE lambda for advantage estimation
      batch_size: 144  # Batch size for each update, if mini-batch approach is used (currently not implemented)
      # Adjust agent information structure
      share_critic: true
      use_base_bid: false
      learn_std: true
      public_info: false
      individual_values: false

  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear


base_mappo_ival_wandb_paramshare1:
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: True

  learning_config:
    experiment_group: "comparison_interim_presentation"
    continue_learning: False
    trained_policies_save_path: null
    max_bid_price: 100
    algorithm: ppo
    device: cpu
    learning_rate: 0.0003 # default learning rate
    learning_rate_schedule: linear
    validation_episodes_interval: 5 # after how many episodes the validation starts and the policy is updated
    training_episodes: 100
    gradient_steps: 10
    matd3:
      actor_architecture: mlp
      train_freq: 24h # how often write_to_learning_role gets called
      episodes_collecting_initial_experience: 3
      batch_size: 64
      gamma: 0.99
      noise_sigma: 0.1
      noise_scale: 1
      noise_dt: 1
    ppo: 
      actor_architecture: dist
      train_freq: 144h # how often write_to_learning_role gets called
      gamma: 0.99 # Discount factor for future rewards
      clip_ratio: 0.05  # Clipping parameter for policy updates
      vf_coef: 0.75  # Value function coefficient in the loss function
      entropy_coef: 0.005  # Entropy coefficient for exploration
      max_grad_norm: 0.3  # Gradient clipping value
      gae_lambda: 0.95  # GAE lambda for advantage estimation
      batch_size: 144  # Batch size for each update, if mini-batch approach is used (currently not implemented)
      # Adjust agent information structure
      share_critic: true
      use_base_bid: false
      learn_std: true
      public_info: true
      individual_values: false

  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear


base_ippo:
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: True

  learning_config:
    experiment_group: "comparison_interim_presentation"
    continue_learning: False
    trained_policies_save_path: null
    max_bid_price: 100
    algorithm: ppo
    device: cpu
    learning_rate: 0.0003 # default learning rate
    learning_rate_schedule: linear
    validation_episodes_interval: 5 # after how many episodes the validation starts and the policy is updated
    training_episodes: 250
    gradient_steps: 5
    matd3:
      actor_architecture: mlp
      train_freq: 24h # how often write_to_learning_role gets called
      episodes_collecting_initial_experience: 3
      batch_size: 64
      gamma: 0.99
      noise_sigma: 0.1
      noise_scale: 1
      noise_dt: 1
    ppo: 
      actor_architecture: dist
      train_freq: 144h # how often write_to_learning_role gets called
      gamma: 0.99 # Discount factor for future rewards
      clip_ratio: 0.05  # Clipping parameter for policy updates
      vf_coef: 0.75  # Value function coefficient in the loss function
      entropy_coef: 0.005  # Entropy coefficient for exploration
      max_grad_norm: 0.3  # Gradient clipping value
      gae_lambda: 0.95  # GAE lambda for advantage estimation
      batch_size: 144  # Batch size for each update, if mini-batch approach is used (currently not implemented)
      # Adjust centralization info
      share_critic: false
      use_base_bid: false
      learn_std: true
      public_info: false


  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear

base_ippo_share_critic_wandb:
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: True

  learning_config:
    experiment_group: "comparison_interim_presentation"
    continue_learning: False
    trained_policies_save_path: null
    max_bid_price: 100
    algorithm: ppo
    device: cpu
    learning_rate: 0.0003 # default learning rate
    learning_rate_schedule: linear
    validation_episodes_interval: 5 # after how many episodes the validation starts and the policy is updated
    training_episodes: 250
    gradient_steps: 10
    matd3:
      actor_architecture: mlp
      train_freq: 24h # how often write_to_learning_role gets called
      episodes_collecting_initial_experience: 3
      batch_size: 64
      gamma: 0.99
      noise_sigma: 0.1
      noise_scale: 1
      noise_dt: 1
    ppo: 
      actor_architecture: dist
      train_freq: 144h # how often write_to_learning_role gets called
      gamma: 0.99 # Discount factor for future rewards
      clip_ratio: 0.05  # Clipping parameter for policy updates
      vf_coef: 0.75  # Value function coefficient in the loss function
      entropy_coef: 0.005  # Entropy coefficient for exploration
      max_grad_norm: 0.3  # Gradient clipping value
      gae_lambda: 0.95  # GAE lambda for advantage estimation
      batch_size: 144  # Batch size for each update, if mini-batch approach is used (currently not implemented)
      # Adjust centralization info
      share_critic: true
      use_base_bid: false
      learn_std: true
      public_info: false

  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear


base_lstm:
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: True

  learning_config:
    continue_learning: False
    trained_policies_save_path: null
    max_bid_price: 100
    algorithm: matd3
    device: cpu
    learning_rate: 0.001
    validation_episodes_interval: 10 # after how many episodes the validation starts and the policy is updated
    training_episodes: 100
    gradient_steps: -1
    matd3:
      actor_architecture: mlp
      train_freq: 24h # how often write_to_learning_role gets called
      episodes_collecting_initial_experience: 3
      batch_size: 64
      gamma: 0.99
      noise_sigma: 0.1
      noise_scale: 1
      noise_dt: 1

  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear


base_mappo_inspect_init:
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: True
 
  learning_config:
    experiment_group: "comparison_interim_presentation"
    continue_learning: False
    trained_policies_save_path: null
    max_bid_price: 100
    algorithm: ppo
    device: cpu
    learning_rate: 0.0002 # default learning rate
    learning_rate_schedule: linear
    validation_episodes_interval: 5 # after how many episodes the validation starts and the policy is updated
    training_episodes: 100
    gradient_steps: 5
    matd3:
      actor_architecture: mlp
      train_freq: 24h # how often write_to_learning_role gets called
      episodes_collecting_initial_experience: 3
      batch_size: 64
      gamma: 0.99
      noise_sigma: 0.1
      noise_scale: 1
      noise_dt: 1
    ppo:
      actor_architecture: dist
      train_freq: 33h # how often write_to_learning_role gets called
      gamma: 0.99 # Discount factor for future rewards
      clip_ratio: 0.05  # Clipping parameter for policy updates
      vf_coef: 0.75  # Value function coefficient in the loss function
      entropy_coef: 0.00  # Entropy coefficient for exploration
      max_grad_norm: 0.3  # Gradient clipping value
      gae_lambda: 0.95  # GAE lambda for advantage estimation
      batch_size: 33  # Batch size for each update, if mini-batch approach is used (currently not implemented)
      # Adjust agent information structure
      share_critic: false
      use_base_bid: false
      learn_std: true
      public_info: true
      individual_values: true
 
  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear
 



base_case_ppo_experimental4:
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: True

  learning_config:
    experiment_group: "comparison_interim_presentation"
    experiment_name: 2b_mappo
    algo_detail: nonormlr0003vclip005
    continue_learning: False
    trained_policies_save_path: null
    max_bid_price: 100
    algorithm: ppo
    device: cuda
    learning_rate: 0.0002 # default learning rate
    # learning_rate_schedule: linear --> no decay to encourage more varied polices
    validation_episodes_interval: 5 # after how many episodes the validation starts and the policy is updated
    training_episodes: 100
    gradient_steps: 10
    matd3:
      actor_architecture: mlp
      train_freq: 24h # how often write_to_learning_role gets called
      episodes_collecting_initial_experience: 3
      batch_size: 64
      gamma: 0.99
      noise_sigma: 0.1
      noise_scale: 1
      noise_dt: 1
    ppo:
      actor_architecture: dist
      train_freq: 360h # how often write_to_learning_role gets called
      gamma: 0.99 # Discount factor for future rewards
      clip_ratio: 0.05  # Clipping parameter for policy updates
      vf_coef: 0.75  # Value function coefficient in the loss function
      entropy_coef: 0.01  # Entropy coefficient for exploration
      max_grad_norm: 3 # Gradient clipping value
      gae_lambda: 0.95  # GAE lambda for advantage estimation
      batch_size: 360  # Batch size for each update, if mini-batch approach is used (currently not implemented)
      # Adjust agent information structure
      share_critic: false
      use_base_bid: false
      learn_std: true
      public_info: true
      individual_values: true
 
  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear



base_case_ppo_experimental4_corrected_buffer:
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: True

  learning_config:
    experiment_group: "comparison_interim_presentation"
    experiment_name: 2b_mappo
    algo_detail: nonormlr0003vclip005
    continue_learning: False
    trained_policies_save_path: null
    max_bid_price: 100
    algorithm: ppo
    device: cuda
    learning_rate: 0.0002 # default learning rate
    # learning_rate_schedule: linear --> no decay to encourage more varied polices
    validation_episodes_interval: 5 # after how many episodes the validation starts and the policy is updated
    training_episodes: 100
    gradient_steps: 10
    matd3:
      actor_architecture: mlp
      train_freq: 24h # how often write_to_learning_role gets called
      episodes_collecting_initial_experience: 3
      batch_size: 64
      gamma: 0.99
      noise_sigma: 0.1
      noise_scale: 1
      noise_dt: 1
    ppo:
      actor_architecture: dist
      train_freq: 360h # how often write_to_learning_role gets called
      gamma: 0.99 # Discount factor for future rewards
      clip_ratio: 0.05  # Clipping parameter for policy updates
      vf_coef: 0.75  # Value function coefficient in the loss function
      entropy_coef: 0.01  # Entropy coefficient for exploration
      max_grad_norm: 3 # Gradient clipping value
      gae_lambda: 0.95  # GAE lambda for advantage estimation
      batch_size: 360  # Batch size for each update, if mini-batch approach is used (currently not implemented)
      # Adjust agent information structure
      share_critic: false
      use_base_bid: false
      learn_std: true
      public_info: true
      individual_values: true
 
  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear


base_case_ppo_experimental4_corrected_buffer_seed42: #use this one for the other seeds too!!: 42, 44, 46
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: True

  learning_config:
    experiment_group: "comparison_interim_presentation"
    experiment_name: 2b_mappo
    algo_detail: nonormlr0003vclip005
    continue_learning: False
    trained_policies_save_path: null
    max_bid_price: 100
    algorithm: ppo
    device: cuda
    learning_rate: 0.0002 # default learning rate
    # learning_rate_schedule: linear --> no decay to encourage more varied polices
    validation_episodes_interval: 5 # after how many episodes the validation starts and the policy is updated
    training_episodes: 100
    gradient_steps: 10
    matd3:
      actor_architecture: mlp
      train_freq: 24h # how often write_to_learning_role gets called
      episodes_collecting_initial_experience: 3
      batch_size: 64
      gamma: 0.99
      noise_sigma: 0.1
      noise_scale: 1
      noise_dt: 1
    ppo:
      actor_architecture: dist
      train_freq: 360h # how often write_to_learning_role gets called
      gamma: 0.99 # Discount factor for future rewards
      clip_ratio: 0.05  # Clipping parameter for policy updates
      vf_coef: 0.75  # Value function coefficient in the loss function
      entropy_coef: 0.01  # Entropy coefficient for exploration
      max_grad_norm: 3 # Gradient clipping value
      gae_lambda: 0.95  # GAE lambda for advantage estimation
      batch_size: 360  # Batch size for each update, if mini-batch approach is used (currently not implemented)
      # Adjust agent information structure
      share_critic: false
      use_base_bid: false
      learn_std: true
      public_info: true
      individual_values: true
 
  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear


base_case_ppo_experimental4_corrected_buffer_seed44: #use this one for the other seeds too!!: 42, 44, 46
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: True

  learning_config:
    experiment_group: "comparison_interim_presentation"
    experiment_name: 2b_mappo
    algo_detail: nonormlr0003vclip005
    continue_learning: False
    trained_policies_save_path: null
    max_bid_price: 100
    algorithm: ppo
    device: cuda
    learning_rate: 0.0002 # default learning rate
    # learning_rate_schedule: linear --> no decay to encourage more varied polices
    validation_episodes_interval: 5 # after how many episodes the validation starts and the policy is updated
    training_episodes: 100
    gradient_steps: 10
    matd3:
      actor_architecture: mlp
      train_freq: 24h # how often write_to_learning_role gets called
      episodes_collecting_initial_experience: 3
      batch_size: 64
      gamma: 0.99
      noise_sigma: 0.1
      noise_scale: 1
      noise_dt: 1
    ppo:
      actor_architecture: dist
      train_freq: 360h # how often write_to_learning_role gets called
      gamma: 0.99 # Discount factor for future rewards
      clip_ratio: 0.05  # Clipping parameter for policy updates
      vf_coef: 0.75  # Value function coefficient in the loss function
      entropy_coef: 0.01  # Entropy coefficient for exploration
      max_grad_norm: 3 # Gradient clipping value
      gae_lambda: 0.95  # GAE lambda for advantage estimation
      batch_size: 360  # Batch size for each update, if mini-batch approach is used (currently not implemented)
      # Adjust agent information structure
      share_critic: false
      use_base_bid: false
      learn_std: true
      public_info: true
      individual_values: true
 
  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear


base_case_ppo_experimental4_corrected_buffer_seed46: #use this one for the other seeds too!!: 42, 44, 46
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: True

  learning_config:
    experiment_group: "comparison_interim_presentation"
    experiment_name: 2b_mappo
    algo_detail: nonormlr0003vclip005
    continue_learning: False
    trained_policies_save_path: null
    max_bid_price: 100
    algorithm: ppo
    device: cuda
    learning_rate: 0.0002 # default learning rate
    # learning_rate_schedule: linear --> no decay to encourage more varied polices
    validation_episodes_interval: 5 # after how many episodes the validation starts and the policy is updated
    training_episodes: 100
    gradient_steps: 10
    matd3:
      actor_architecture: mlp
      train_freq: 24h # how often write_to_learning_role gets called
      episodes_collecting_initial_experience: 3
      batch_size: 64
      gamma: 0.99
      noise_sigma: 0.1
      noise_scale: 1
      noise_dt: 1
    ppo:
      actor_architecture: dist
      train_freq: 360h # how often write_to_learning_role gets called
      gamma: 0.99 # Discount factor for future rewards
      clip_ratio: 0.05  # Clipping parameter for policy updates
      vf_coef: 0.75  # Value function coefficient in the loss function
      entropy_coef: 0.01  # Entropy coefficient for exploration
      max_grad_norm: 3 # Gradient clipping value
      gae_lambda: 0.95  # GAE lambda for advantage estimation
      batch_size: 360  # Batch size for each update, if mini-batch approach is used (currently not implemented)
      # Adjust agent information structure
      share_critic: false
      use_base_bid: false
      learn_std: true
      public_info: true
      individual_values: true
 
  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear





base_case_ppo_experimental4_corrected_buffer_no_mc_init:
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: True

  learning_config:
    experiment_group: "comparison_interim_presentation"
    experiment_name: 2b_mappo
    algo_detail: nonormlr0003vclip005
    continue_learning: False
    trained_policies_save_path: null
    max_bid_price: 100
    algorithm: ppo
    device: cuda
    learning_rate: 0.0002 # default learning rate
    # learning_rate_schedule: linear --> no decay to encourage more varied polices
    validation_episodes_interval: 5 # after how many episodes the validation starts and the policy is updated
    training_episodes: 100
    gradient_steps: 10
    matd3:
      actor_architecture: mlp
      train_freq: 24h # how often write_to_learning_role gets called
      episodes_collecting_initial_experience: 3
      batch_size: 64
      gamma: 0.99
      noise_sigma: 0.1
      noise_scale: 1
      noise_dt: 1
    ppo:
      actor_architecture: dist
      train_freq: 360h # how often write_to_learning_role gets called
      gamma: 0.99 # Discount factor for future rewards
      clip_ratio: 0.05  # Clipping parameter for policy updates
      vf_coef: 0.75  # Value function coefficient in the loss function
      entropy_coef: 0.01  # Entropy coefficient for exploration
      max_grad_norm: 3 # Gradient clipping value
      gae_lambda: 0.95  # GAE lambda for advantage estimation
      batch_size: 360  # Batch size for each update, if mini-batch approach is used (currently not implemented)
      # Adjust agent information structure
      share_critic: false
      use_base_bid: false
      learn_std: true
      public_info: true
      individual_values: true
 
  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear


base_case_ippo_experimental_corrected_buffer_and_obs:
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: True

  learning_config:
    experiment_group: "comparison_interim_presentation"
    experiment_name: 2b_mappo
    algo_detail: nonormlr0003vclip005
    continue_learning: False
    trained_policies_save_path: null
    max_bid_price: 100
    algorithm: ppo
    device: cuda
    learning_rate: 0.0002 # default learning rate
    # learning_rate_schedule: linear --> no decay to encourage more varied polices
    validation_episodes_interval: 5 # after how many episodes the validation starts and the policy is updated
    training_episodes: 100
    gradient_steps: 10
    matd3:
      actor_architecture: mlp
      train_freq: 24h # how often write_to_learning_role gets called
      episodes_collecting_initial_experience: 3
      batch_size: 64
      gamma: 0.99
      noise_sigma: 0.1
      noise_scale: 1
      noise_dt: 1
    ppo:
      actor_architecture: dist
      train_freq: 360h # how often write_to_learning_role gets called
      gamma: 0.99 # Discount factor for future rewards
      clip_ratio: 0.05  # Clipping parameter for policy updates
      vf_coef: 0.75  # Value function coefficient in the loss function
      entropy_coef: 0.01  # Entropy coefficient for exploration
      max_grad_norm: 3 # Gradient clipping value
      gae_lambda: 0.95  # GAE lambda for advantage estimation
      batch_size: 360  # Batch size for each update, if mini-batch approach is used (currently not implemented)
      # Adjust agent information structure
      share_critic: false
      use_base_bid: false
      learn_std: true
      public_info: false
      individual_values: true
 
  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear





base_case_Ippo_experimental4_corrected_buffer_seed42: #use this one for the other seeds too!!: 42, 44, 46
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: True

  learning_config:
    experiment_group: "comparison_interim_presentation"
    experiment_name: 2b_mappo
    algo_detail: nonormlr0003vclip005
    continue_learning: False
    trained_policies_save_path: null
    max_bid_price: 100
    algorithm: ppo
    device: cuda
    learning_rate: 0.0002 # default learning rate
    # learning_rate_schedule: linear --> no decay to encourage more varied polices
    validation_episodes_interval: 5 # after how many episodes the validation starts and the policy is updated
    training_episodes: 100
    gradient_steps: 10
    matd3:
      actor_architecture: mlp
      train_freq: 24h # how often write_to_learning_role gets called
      episodes_collecting_initial_experience: 3
      batch_size: 64
      gamma: 0.99
      noise_sigma: 0.1
      noise_scale: 1
      noise_dt: 1
    ppo:
      actor_architecture: dist
      train_freq: 360h # how often write_to_learning_role gets called
      gamma: 0.99 # Discount factor for future rewards
      clip_ratio: 0.05  # Clipping parameter for policy updates
      vf_coef: 0.75  # Value function coefficient in the loss function
      entropy_coef: 0.01  # Entropy coefficient for exploration
      max_grad_norm: 3 # Gradient clipping value
      gae_lambda: 0.95  # GAE lambda for advantage estimation
      batch_size: 360  # Batch size for each update, if mini-batch approach is used (currently not implemented)
      # Adjust agent information structure
      share_critic: false
      use_base_bid: false
      learn_std: true
      public_info: false
      individual_values: true
 
  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear


base_case_Ippo_experimental4_corrected_buffer_seed44: #use this one for the other seeds too!!: 42, 44, 46
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: True

  learning_config:
    experiment_group: "comparison_interim_presentation"
    experiment_name: 2b_mappo
    algo_detail: nonormlr0003vclip005
    continue_learning: False
    trained_policies_save_path: null
    max_bid_price: 100
    algorithm: ppo
    device: cuda
    learning_rate: 0.0002 # default learning rate
    # learning_rate_schedule: linear --> no decay to encourage more varied polices
    validation_episodes_interval: 5 # after how many episodes the validation starts and the policy is updated
    training_episodes: 100
    gradient_steps: 10
    matd3:
      actor_architecture: mlp
      train_freq: 24h # how often write_to_learning_role gets called
      episodes_collecting_initial_experience: 3
      batch_size: 64
      gamma: 0.99
      noise_sigma: 0.1
      noise_scale: 1
      noise_dt: 1
    ppo:
      actor_architecture: dist
      train_freq: 360h # how often write_to_learning_role gets called
      gamma: 0.99 # Discount factor for future rewards
      clip_ratio: 0.05  # Clipping parameter for policy updates
      vf_coef: 0.75  # Value function coefficient in the loss function
      entropy_coef: 0.01  # Entropy coefficient for exploration
      max_grad_norm: 3 # Gradient clipping value
      gae_lambda: 0.95  # GAE lambda for advantage estimation
      batch_size: 360  # Batch size for each update, if mini-batch approach is used (currently not implemented)
      # Adjust agent information structure
      share_critic: false
      use_base_bid: false
      learn_std: true
      public_info: false
      individual_values: true
 
  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear



base_case_Ippo_experimental4_corrected_buffer_seed46: #use this one for the other seeds too!!: 42, 44, 46
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: True

  learning_config:
    experiment_group: "comparison_interim_presentation"
    experiment_name: 2b_mappo
    algo_detail: nonormlr0003vclip005
    continue_learning: False
    trained_policies_save_path: null
    max_bid_price: 100
    algorithm: ppo
    device: cuda
    learning_rate: 0.0002 # default learning rate
    # learning_rate_schedule: linear --> no decay to encourage more varied polices
    validation_episodes_interval: 5 # after how many episodes the validation starts and the policy is updated
    training_episodes: 100
    gradient_steps: 10
    matd3:
      actor_architecture: mlp
      train_freq: 24h # how often write_to_learning_role gets called
      episodes_collecting_initial_experience: 3
      batch_size: 64
      gamma: 0.99
      noise_sigma: 0.1
      noise_scale: 1
      noise_dt: 1
    ppo:
      actor_architecture: dist
      train_freq: 360h # how often write_to_learning_role gets called
      gamma: 0.99 # Discount factor for future rewards
      clip_ratio: 0.05  # Clipping parameter for policy updates
      vf_coef: 0.75  # Value function coefficient in the loss function
      entropy_coef: 0.01  # Entropy coefficient for exploration
      max_grad_norm: 3 # Gradient clipping value
      gae_lambda: 0.95  # GAE lambda for advantage estimation
      batch_size: 360  # Batch size for each update, if mini-batch approach is used (currently not implemented)
      # Adjust agent information structure
      share_critic: false
      use_base_bid: false
      learn_std: true
      public_info: false
      individual_values: true
 
  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear



base_case_ppo_paramshare_experimental4_corrected_buffer_seed42: #use this one for the other seeds too!!: 42, 44, 46
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: True

  learning_config:
    experiment_group: "comparison_interim_presentation"
    experiment_name: 2b_mappo
    algo_detail: nonormlr0003vclip005
    continue_learning: False
    trained_policies_save_path: null
    max_bid_price: 100
    algorithm: ppo
    device: cuda
    learning_rate: 0.0002 # default learning rate
    # learning_rate_schedule: linear --> no decay to encourage more varied polices
    validation_episodes_interval: 5 # after how many episodes the validation starts and the policy is updated
    training_episodes: 100
    gradient_steps: 10
    matd3:
      actor_architecture: mlp
      train_freq: 24h # how often write_to_learning_role gets called
      episodes_collecting_initial_experience: 3
      batch_size: 64
      gamma: 0.99
      noise_sigma: 0.1
      noise_scale: 1
      noise_dt: 1
    ppo:
      actor_architecture: dist
      train_freq: 360h # how often write_to_learning_role gets called
      gamma: 0.99 # Discount factor for future rewards
      clip_ratio: 0.05  # Clipping parameter for policy updates
      vf_coef: 0.75  # Value function coefficient in the loss function
      entropy_coef: 0.01  # Entropy coefficient for exploration
      max_grad_norm: 3 # Gradient clipping value
      gae_lambda: 0.95  # GAE lambda for advantage estimation
      batch_size: 360  # Batch size for each update, if mini-batch approach is used (currently not implemented)
      # Adjust agent information structure
      share_critic: true
      use_base_bid: false
      learn_std: true
      public_info: true
      individual_values: false
 
  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear


base_case_ppo_paramshare_experimental4_corrected_buffer_seed44: #use this one for the other seeds too!!: 42, 44, 46
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: True

  learning_config:
    experiment_group: "comparison_interim_presentation"
    experiment_name: 2b_mappo
    algo_detail: nonormlr0003vclip005
    continue_learning: False
    trained_policies_save_path: null
    max_bid_price: 100
    algorithm: ppo
    device: cuda
    learning_rate: 0.0002 # default learning rate
    # learning_rate_schedule: linear --> no decay to encourage more varied polices
    validation_episodes_interval: 5 # after how many episodes the validation starts and the policy is updated
    training_episodes: 100
    gradient_steps: 10
    matd3:
      actor_architecture: mlp
      train_freq: 24h # how often write_to_learning_role gets called
      episodes_collecting_initial_experience: 3
      batch_size: 64
      gamma: 0.99
      noise_sigma: 0.1
      noise_scale: 1
      noise_dt: 1
    ppo:
      actor_architecture: dist
      train_freq: 360h # how often write_to_learning_role gets called
      gamma: 0.99 # Discount factor for future rewards
      clip_ratio: 0.05  # Clipping parameter for policy updates
      vf_coef: 0.75  # Value function coefficient in the loss function
      entropy_coef: 0.01  # Entropy coefficient for exploration
      max_grad_norm: 3 # Gradient clipping value
      gae_lambda: 0.95  # GAE lambda for advantage estimation
      batch_size: 360  # Batch size for each update, if mini-batch approach is used (currently not implemented)
      # Adjust agent information structure
      share_critic: true
      use_base_bid: false
      learn_std: true
      public_info: true
      individual_values: false
 
  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear


base_case_ppo_paramshare_experimental4_corrected_buffer_seed46: #use this one for the other seeds too!!: 42, 44, 46
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: True

  learning_config:
    experiment_group: "comparison_interim_presentation"
    experiment_name: 2b_mappo
    algo_detail: nonormlr0003vclip005
    continue_learning: False
    trained_policies_save_path: null
    max_bid_price: 100
    algorithm: ppo
    device: cuda
    learning_rate: 0.0002 # default learning rate
    # learning_rate_schedule: linear --> no decay to encourage more varied polices
    validation_episodes_interval: 5 # after how many episodes the validation starts and the policy is updated
    training_episodes: 100
    gradient_steps: 10
    matd3:
      actor_architecture: mlp
      train_freq: 24h # how often write_to_learning_role gets called
      episodes_collecting_initial_experience: 3
      batch_size: 64
      gamma: 0.99
      noise_sigma: 0.1
      noise_scale: 1
      noise_dt: 1
    ppo:
      actor_architecture: dist
      train_freq: 360h # how often write_to_learning_role gets called
      gamma: 0.99 # Discount factor for future rewards
      clip_ratio: 0.05  # Clipping parameter for policy updates
      vf_coef: 0.75  # Value function coefficient in the loss function
      entropy_coef: 0.01  # Entropy coefficient for exploration
      max_grad_norm: 3 # Gradient clipping value
      gae_lambda: 0.95  # GAE lambda for advantage estimation
      batch_size: 360  # Batch size for each update, if mini-batch approach is used (currently not implemented)
      # Adjust agent information structure
      share_critic: true
      use_base_bid: false
      learn_std: true
      public_info: true
      individual_values: false
 
  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear



base_case_Ippo_paramshare_experimental4_corrected_buffer_seed42: #use this one for the other seeds too!!: 42, 44, 46
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: True

  learning_config:
    experiment_group: "comparison_interim_presentation"
    experiment_name: 2b_mappo
    algo_detail: nonormlr0003vclip005
    continue_learning: False
    trained_policies_save_path: null
    max_bid_price: 100
    algorithm: ppo
    device: cuda
    learning_rate: 0.0002 # default learning rate
    # learning_rate_schedule: linear --> no decay to encourage more varied polices
    validation_episodes_interval: 5 # after how many episodes the validation starts and the policy is updated
    training_episodes: 100
    gradient_steps: 10
    matd3:
      actor_architecture: mlp
      train_freq: 24h # how often write_to_learning_role gets called
      episodes_collecting_initial_experience: 3
      batch_size: 64
      gamma: 0.99
      noise_sigma: 0.1
      noise_scale: 1
      noise_dt: 1
    ppo:
      actor_architecture: dist
      train_freq: 360h # how often write_to_learning_role gets called
      gamma: 0.99 # Discount factor for future rewards
      clip_ratio: 0.05  # Clipping parameter for policy updates
      vf_coef: 0.75  # Value function coefficient in the loss function
      entropy_coef: 0.01  # Entropy coefficient for exploration
      max_grad_norm: 3 # Gradient clipping value
      gae_lambda: 0.95  # GAE lambda for advantage estimation
      batch_size: 360  # Batch size for each update, if mini-batch approach is used (currently not implemented)
      # Adjust agent information structure
      share_critic: true
      use_base_bid: false
      learn_std: true
      public_info: false
      individual_values: false
 
  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear


base_case_Ippo_paramshare_experimental4_corrected_buffer_seed44: #use this one for the other seeds too!!: 42, 44, 46
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: True

  learning_config:
    experiment_group: "comparison_interim_presentation"
    experiment_name: 2b_mappo
    algo_detail: nonormlr0003vclip005
    continue_learning: False
    trained_policies_save_path: null
    max_bid_price: 100
    algorithm: ppo
    device: cuda
    learning_rate: 0.0002 # default learning rate
    # learning_rate_schedule: linear --> no decay to encourage more varied polices
    validation_episodes_interval: 5 # after how many episodes the validation starts and the policy is updated
    training_episodes: 100
    gradient_steps: 10
    matd3:
      actor_architecture: mlp
      train_freq: 24h # how often write_to_learning_role gets called
      episodes_collecting_initial_experience: 3
      batch_size: 64
      gamma: 0.99
      noise_sigma: 0.1
      noise_scale: 1
      noise_dt: 1
    ppo:
      actor_architecture: dist
      train_freq: 360h # how often write_to_learning_role gets called
      gamma: 0.99 # Discount factor for future rewards
      clip_ratio: 0.05  # Clipping parameter for policy updates
      vf_coef: 0.75  # Value function coefficient in the loss function
      entropy_coef: 0.01  # Entropy coefficient for exploration
      max_grad_norm: 3 # Gradient clipping value
      gae_lambda: 0.95  # GAE lambda for advantage estimation
      batch_size: 360  # Batch size for each update, if mini-batch approach is used (currently not implemented)
      # Adjust agent information structure
      share_critic: true
      use_base_bid: false
      learn_std: true
      public_info: false
      individual_values: false
 
  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear


base_case_Ippo_paramshare_experimental4_corrected_buffer_seed46: #use this one for the other seeds too!!: 42, 44, 46
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: True

  learning_config:
    experiment_group: "comparison_interim_presentation"
    experiment_name: 2b_mappo
    algo_detail: nonormlr0003vclip005
    continue_learning: False
    trained_policies_save_path: null
    max_bid_price: 100
    algorithm: ppo
    device: cuda
    learning_rate: 0.0002 # default learning rate
    # learning_rate_schedule: linear --> no decay to encourage more varied polices
    validation_episodes_interval: 5 # after how many episodes the validation starts and the policy is updated
    training_episodes: 100
    gradient_steps: 10
    matd3:
      actor_architecture: mlp
      train_freq: 24h # how often write_to_learning_role gets called
      episodes_collecting_initial_experience: 3
      batch_size: 64
      gamma: 0.99
      noise_sigma: 0.1
      noise_scale: 1
      noise_dt: 1
    ppo:
      actor_architecture: dist
      train_freq: 360h # how often write_to_learning_role gets called
      gamma: 0.99 # Discount factor for future rewards
      clip_ratio: 0.05  # Clipping parameter for policy updates
      vf_coef: 0.75  # Value function coefficient in the loss function
      entropy_coef: 0.01  # Entropy coefficient for exploration
      max_grad_norm: 3 # Gradient clipping value
      gae_lambda: 0.95  # GAE lambda for advantage estimation
      batch_size: 360  # Batch size for each update, if mini-batch approach is used (currently not implemented)
      # Adjust agent information structure
      share_critic: true
      use_base_bid: false
      learn_std: true
      public_info: false
      individual_values: false
 
  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear


 
base_ppo_lstm_experimental_seed42:
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: true
  learning_config:
    algorithm: ppo
    continue_learning: false
    device: cpu
    gradient_steps: 5
    learning_rate: 0.0001
    #learning_rate_schedule: linear
    validation_episodes_interval: 5 # after how many episodes the validation starts and the policy is updated
    training_episodes: 100
    max_bid_price: 100
    matd3:
      actor_architecture: mlp
      batch_size: 64
      episodes_collecting_initial_experience: 0
      gamma: 0.99
      noise_dt: 1
      noise_scale: 1
      noise_sigma: 0.1
      train_freq: 24h
    ppo:
      actor_architecture: distlstm
      train_freq: 360h # how often write_to_learning_role gets called
      gamma: 0.99 # Discount factor for future rewards
      clip_ratio: 0.05  # Clipping parameter for policy updates
      vf_coef: 0.75  # Value function coefficient in the loss function
      entropy_coef: 0.01  # Entropy coefficient for exploration
      max_grad_norm: 3 # Gradient clipping value
      gae_lambda: 0.95  # GAE lambda for advantage estimation
      batch_size: 360  # Batch size for each update, if mini-batch approach is used (currently not implemented)
      # Adjust agent information structure
      # Adjust agent information structure
      share_critic: false
      #use_base_bid: false
      learn_std: false
      public_info: true
      individual_values: true
 
  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear




base_ppo_lstm_experimental_seed42:
  start_date: 2019-03-01 00:00
  end_date: 2019-04-01 00:00
  time_step: 1h
  save_frequency_hours: null
  learning_mode: False
  perform_evaluation: True
  continue_learning: True

  trained_policies_load_path: 'examples\inputs\example_02b\learned_strategies\base_ppo_lstm_experimental_seed42\last_policies'  

  learning_config:
    algorithm: ppo
    continue_learning: false
    trained_policies_load_path: 'examples\inputs\example_02b\learned_strategies\base_ppo_lstm_experimental_seed42\last_policies'  
    device: cpu
    gradient_steps: 5
    learning_rate: 0.0001
    #learning_rate_schedule: linear
    validation_episodes_interval: 5 # after how many episodes the validation starts and the policy is updated
    training_episodes: 100
    max_bid_price: 100
    matd3:
      actor_architecture: mlp
      batch_size: 64
      episodes_collecting_initial_experience: 0
      gamma: 0.99
      noise_dt: 1
      noise_scale: 1
      noise_sigma: 0.1
      train_freq: 24h
    ppo:
      actor_architecture: distlstm
      train_freq: 360h # how often write_to_learning_role gets called
      gamma: 0.99 # Discount factor for future rewards
      clip_ratio: 0.05  # Clipping parameter for policy updates
      vf_coef: 0.75  # Value function coefficient in the loss function
      entropy_coef: 0.01  # Entropy coefficient for exploration
      max_grad_norm: 3 # Gradient clipping value
      gae_lambda: 0.95  # GAE lambda for advantage estimation
      batch_size: 360  # Batch size for each update, if mini-batch approach is used (currently not implemented)
      # Adjust agent information structure
      # Adjust agent information structure
      share_critic: false
      #use_base_bid: false
      learn_std: false
      public_info: true
      individual_values: true
 
  markets_config:
    EOM:
      operator: EOM_operator
      product_type: energy
      products:
        - duration: 1h
          count: 1
          first_delivery: 1h
      opening_frequency: 1h
      opening_duration: 1h
      volume_unit: MWh
      maximum_bid_volume: 100000
      maximum_bid_price: 3000
      minimum_bid_price: -500
      price_unit: EUR/MWh
      market_mechanism: pay_as_clear
 